{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g1q1mnorfHwx"
   },
   "source": [
    "<div style=\"width: 100%; clear: both;\">\n",
    "<div style=\"float: left; width: 50%;\">\n",
    "<img src=\"https://www.uoc.edu/content/dam/news/images/noticies/2016/202-nova-marca-uoc.jpg\" align=\"left\" width=\"45%\">\n",
    "</div>\n",
    "<div style=\"float: right; width: 50%;\">\n",
    "<p style=\"margin: 0; padding-top: 22px; text-align:right;\">M2.891 췅 Aprendizaje autom치tico 췅 PEC3</p>\n",
    "<p style=\"margin: 0; text-align:right;\">2024-1 췅 M치ster universitario en Ciencia de datos (Data science)</p>\n",
    "<p style=\"margin: 0; text-align:right; padding-button: 100px;\">Estudios de Inform치tica, Multimedia y Telecomunicaci칩n</p>\n",
    "</div>\n",
    "</div>\n",
    "<div style=\"width:100%;\">&nbsp;</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFTsh39afHwz"
   },
   "source": [
    "# PEC 3: M칠todos supervisados\n",
    "\n",
    "En esta pr치ctica veremos diferentes m칠todos supervisados y trataremos de optimizar diferentes m칠tricas. Veremos como los diferentes modelos clasifican las observaciones y con cuales obtenemos mayor rendimiento. Despu칠s aplicaremos todo lo que hemos aprendido hasta ahora a un dataset nuevo simulando un caso pr치ctico real.\n",
    "\n",
    "1. [Exploraci칩n de algoritmos supervisados](#eje1) \\\n",
    "    1.1. [Carga de datos](#eje10) \\\n",
    "    1.2. [Naive-Bayes](#eje11) \\\n",
    "    1.3. [An치lisis Discriminante Lineal (LDA) y An치lisis Discriminante Cuadrt치tico (QDA)](#eje12) \\\n",
    "    1.4. [K vecinos m치s pr칩ximos (KNN)](#eje13) \\\n",
    "    1.5. [M치quinas de soporte vectorial (SVM)](#eje14) \\\n",
    "    1.6. [츼rboles de decisi칩n](#eje15) \n",
    "2. [Implementaci칩n del caso pr치ctico](#eje2)\\\n",
    "    2.1. [Carga de datos](#eje20) \\\n",
    "    2.2. [An치lisis Exploratorio de Datos](#eje21) \\\n",
    "    2.3. [Preprocesamiento de Datos](#eje22) \\\n",
    "    2.4. [Modelizaci칩n](#eje23) \n",
    "\n",
    "\n",
    "<u>Consideraciones generales</u>:\n",
    "\n",
    "- La soluci칩n planteada no puede utilizar m칠todos, funciones o par치metros declarados **_deprecated_** en futuras versiones, a excepci칩n de la carga de datos c칩mo se indica posteriormente.\n",
    "- Esta PEC debe realizarse de forma **estrictamente individual**. Cualquier indicio de copia ser치 penalizado con un suspenso (D) para todas las partes implicadas y la posible evaluaci칩n negativa de la asignatura de forma 칤ntegra.\n",
    "- Es necesario que el estudiante indique **todas las fuentes** que ha utilizado para la realizaci칩n de la PEC. De no ser as칤, se considerar치 que el estudiante ha cometido plagio, siendo penalizado con un suspenso (D) y la posible evaluaci칩n negativa de la asignatura de forma 칤ntegra.\n",
    "\n",
    "<u>Formato de la entrega</u>:\n",
    "\n",
    "- Algunos ejercicios pueden suponer varios minutos de ejecuci칩n, por lo que la entrega debe hacerse en **formato notebook** y en **formato html**, donde se vea el c칩digo, los resultados y comentarios de cada ejercicio. Se puede exportar el notebook a HTML desde el men칰 File $\\to$ Download as $\\to$ HTML.\n",
    "- Existe un tipo de celda especial para albergar texto. Este tipo de celda os ser치 muy 칰til para responder a las diferentes preguntas te칩ricas planteadas a lo largo de la actividad. Para cambiar el tipo de celda a este tipo, en el men칰: Cell $\\to$ Cell Type $\\to$ Markdown."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DSSB2tHkfHw0"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Nombre y apellidos:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bn0ZMDyKfHw0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shap\n",
    "import copy\n",
    "import tqdm\n",
    "import torch\n",
    "import pickle\n",
    "import kagglehub\n",
    "import umap\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from torcheval.metrics.functional import binary_f1_score, binary_accuracy, binary_auroc\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIkGtp_BfHw0"
   },
   "source": [
    "<a id='ej1'></a>\n",
    "# 1. Exploraci칩n de algoritmos supervisados\n",
    "\n",
    "## 1.1. Carga de datos\n",
    "\n",
    "El conjunto de datos Fashion MNIST proporcionado por Zalando consta de 70.000 im치genes con 10 clases diferentes de ropa repartidas uniformemente. No obstante, para esta pr치ctica utilizaremos 칰nicamente un subconjunto de 5.000 im치genes que consiste en 1.000 im치genes de 5 clases diferentes.\n",
    "\n",
    "Las im치genes tienen una resoluci칩n de 28x28 p칤xeles en escala de grises, por lo que se pueden representar utilizando un vector de 784 posiciones.\n",
    "\n",
    "El siguiente c칩digo cargar치 las 5.000 im치genes en la variable images y las correspondientes etiquetas (en forma num칠rica) en la variable labels. Podemos comprobar que la carga ha sido correcta obteniendo las dimensiones de estas dos variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 465
    },
    "id": "MPEdyuVLfHw1",
    "outputId": "34a46905-a29c-4263-9161-b2fd1efd6276"
   },
   "outputs": [],
   "source": [
    "with open(\"data.pickle\", \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "    \n",
    "X = data[\"images\"]\n",
    "y = data[\"labels\"]\n",
    "n_classes = 5\n",
    "labels = [\"T-shirt\", \"Trouser\", \"Pullover\", \"Dress\", \"Coat\"]\n",
    "\n",
    "print(\"Vector Image Dimensions: {}\".format(X.shape))\n",
    "print(\"Vector Label Dimensions: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "81-zMw2NfHw1"
   },
   "source": [
    "Con el siguiente c칩digo podemos ver un ejemplo de imagen de cada una de las clases. Para ello reajustamos el vector de 784 dimensiones que representa cada imagen en una matriz de tama침o 28x28 y la transponemos para mostrarla:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, n_classes, figsize=(10,10))\n",
    "\n",
    "idxs = [np.where(y == i)[0] for i in range(n_classes)]\n",
    "\n",
    "for i in range(n_classes):\n",
    "    k = np.random.choice(idxs[i])\n",
    "    ax[i].imshow(X[k].reshape(28, 28), cmap=\"gray\")\n",
    "    ax[i].set_title(\"{}\".format(labels[i]))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementaci칩n:</strong> \n",
    "\n",
    "Dividid el _dataset_ en dos subconjuntos, __*train*__ (80% de los datos) y __*test*__ (20% de los datos). Nombrad los conjuntos como: X_train, X_test, y_train, y_test. Utilizad la opci칩n `random_state = 24`.\n",
    "    \n",
    "Pod칠is utilizar la implementaci칩n `train_test_split` de `sklearn`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para poder visualizar los resultados de cada algoritmo supervisado, reduciremos el dataset anterior a dos dimensiones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = umap.UMAP(n_components=2, random_state=42)\n",
    "model.fit(X_train)\n",
    "X_train_projection = model.transform(X_train)\n",
    "X_test_projection = model.transform(X_test)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(10, 8))\n",
    "for i in range(n_classes):\n",
    "    ax.scatter(X_test_projection[y_test == i,0], X_test_projection[y_test == i,1], s=3, label=labels[i])\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A lo largo de los ejercicios aprenderemos a ver gr치ficamente las fronteras de decisi칩n que nos devuelven los diferentes modelos. Para ello utilizaremos la funci칩n definida a continuaci칩n, que sigue los siguientes pasos:\n",
    "\n",
    "Crear una meshgrid con los valores m칤nimo y m치ximo de 'x' e 'y'.\n",
    "Predecir el clasificador con los valores de la meshgrid.\n",
    "Hacer un reshape de los datos para tener el formato correspondiente.\n",
    "Una vez hecho esto, ya podemos hacer el gr치fico de las fronteras de decisi칩n y a침adir los puntos reales. As칤 veremos las 치reas que el modelo considera que son de una clase y las que considera que son de otra. Al poner encima los puntos veremos si los clasifica correctamente en el 치rea que les corresponde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the meshgrid with the minimum and maximum values of the x and y axes\n",
    "x_min, x_max = X_test_projection[:, 0].min() - 1, X_test_projection[:, 0].max() + 1\n",
    "y_min, y_max = X_test_projection[:, 1].min() - 1, X_test_projection[:, 1].max() + 1\n",
    "\n",
    "# Define the function that will visualize the decision boundary\n",
    "def plot_decision_boundaries(model, X_test_projection, y_test):\n",
    "    \n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.05),\n",
    "                         np.arange(y_min, y_max, 0.05))\n",
    "    \n",
    "    # Prediction by using all values of the meshgrid\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "\n",
    "    # Define the colors (one for each class)\n",
    "    cmap_light = ListedColormap(['gainsboro','lightgreen','peachpuff','lightcyan', 'pink'])\n",
    "    cmap_bold = ['grey','g','sandybrown','c','palevioletred']\n",
    "    \n",
    "    # Draw the borders\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.pcolormesh(xx, yy, Z, cmap=cmap_light)\n",
    "\n",
    "    # Draw the points\n",
    "    for i in range(n_classes):\n",
    "        plt.scatter(X_test_projection[y_test == i,0], X_test_projection[y_test == i,1], \n",
    "                    s=3, label=labels[i], c=cmap_bold[i])\n",
    "    plt.legend()\n",
    "    plt.xlim(xx.min(), xx.max())\n",
    "    plt.ylim(yy.min(), yy.max())\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2k77YKbhfHw2"
   },
   "source": [
    "<a id='ej11'></a>\n",
    "## 1.2. Gaussian Na칦ve Bayes (1 punto)\n",
    "\n",
    "El prop칩sito de este primer ejercicio es comprender el funcionamiento del algoritmo Na칦ve-Bayes, un algoritmo peculiar que se basa en el teorema de Bayes para calcular la probabilidad de que una observaci칩n pertenezca a cada una de las clases. Este modelo asume que las caracter칤sticas de entrada son independientes entre s칤, lo que permite simplificar el c치lculo de las probabilidades condicionales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sYiQpphdfHw2"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "1. **Entrena un Modelo de Na칦ve-Bayes:** Utiliza el conjunto de datos de _train_ para entrenar un modelo de Na칦ve-Bayes. Emplea el clasificador `GaussianNB` de la biblioteca `sklearn` para este fin.\n",
    "\n",
    "2. **Calcula el _Accuracy_ del Modelo:** Una vez entrenado el modelo, calcula su precisi칩n (_accuracy_) tanto en el conjunto de _train_ como en el de _test_. Esto te permitir치 evaluar qu칠 tan bien est치 funcionando tu modelo.\n",
    "\n",
    "3. **Calcula la Matriz de Confusi칩n:** Utiliza el conjunto de _test_ para calcular la matriz de confusi칩n del modelo. Esta matriz te ayudar치 a entender de mejor manera los aciertos y errores de tu clasificador.\n",
    "\n",
    "4. **Representa Gr치ficamente la Frontera de Decisi칩n:** Finalmente, visualiza la frontera de decisi칩n del modelo utilizando el conjunto de _test_. Puedes hacer esto con la ayuda de la funci칩n `plot_decision_boundary` que ya has creado previamente.\n",
    "\n",
    "Para realizar estos c치lculos y visualizaciones, utiliza las funciones `accuracy_score` y `confusion_matrix` del paquete `metrics` de `sklearn`.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cRhVuBpCfHw2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "44uFc_xEfHw2"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6EB_a4o-fHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong> \n",
    "  \n",
    "An치lisis del ejercicio.\n",
    "\n",
    "   - 쮺칩mo son las fronteras de decisi칩n? 쯊iene sentido que tengan esta forma con el algoritmo utilizado?\n",
    "   - 쮺칩mo son las predicciones obtenidas sobre el conjunto de test?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mAwzg5BafHw3",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I32OtF0CfHw3"
   },
   "source": [
    "<a id='ej12'></a>\n",
    "## 1.3 An치lisis Discriminante Lineal (LDA) y An치lisis Discriminante Cuadr치tico (QDA) (1 punto)\n",
    "\n",
    "Ahora, analizar치s dos algoritmos que se basan en la transformaci칩n lineal de las caracter칤sticas de entrada para maximizar la separaci칩n entre las clases. Estos modelos operan bajo la suposici칩n de que las caracter칤sticas siguen una distribuci칩n gaussiana. Esto te permitir치 calcular las probabilidades condicionales de cada clase. Con estos c치lculos, asignar치s a cada observaci칩n la clase que presente la mayor probabilidad condicional."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmyzyqT6fHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Sigue estos pasos con el dataset de entrenamiento (_train_):    \n",
    "    \n",
    "1. Entrena un modelo de An치lisis Discriminante Lineal (LDA) utilizando el clasificador `LinearDiscriminantAnalysis` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "\n",
    "Estas acciones te ayudar치n a evaluar la eficacia del modelo LDA en tu conjunto de datos y a entender mejor c칩mo clasifica las observaciones.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0mXPWfXgfHw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QLxfTyOvfHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CWXabsd9fHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Observa las fronteras de decisi칩n que has generado. Reflexiona sobre su forma: 쯉e ajustan a lo que esperar칤as del algoritmo de An치lisis Discriminante Lineal (LDA)? Considera la naturaleza lineal del algoritmo y c칩mo esto influye en la forma de las fronteras.\n",
    "2. Eval칰a las predicciones realizadas sobre el conjunto de test. Analiza su precisi칩n y c칩mo se distribuyen respecto a las fronteras de decisi칩n. 쯉on coherentes estas predicciones con lo que observas en las fronteras de decisi칩n?\n",
    "\n",
    "Estas reflexiones te permitir치n comprender mejor la efectividad del modelo LDA y su adecuaci칩n para el conjunto de datos con el que est치s trabajando.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_7v6A60kfHw3",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K5IEnCibfHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Realiza los siguientes pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un modelo de An치lisis Discriminante Cuadr치tico (QDA) usando el clasificador `QuadraticDiscriminantAnalysis` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "\n",
    "Estos pasos te ayudar치n a evaluar c칩mo el modelo QDA se comporta con tu conjunto de datos, y a entender su capacidad para clasificar y diferenciar entre las clases.\"\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M0VaPQNffHw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhNPVO3CfHw3"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aDneGeZhssYx"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l50NzPaTssYx"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Examina las fronteras de decisi칩n que has generado. Reflexiona sobre su forma: 쮼s coherente con lo que esperar칤as del algoritmo de An치lisis Discriminante Cuadr치tico (QDA)? Considera c칩mo la naturaleza cuadr치tica del algoritmo podr칤a influir en la forma de estas fronteras.\n",
    "2. Eval칰a las predicciones realizadas sobre el conjunto de test. Observa su precisi칩n y c칩mo se distribuyen en relaci칩n con las fronteras de decisi칩n. 쯉on estas predicciones consistentes con las fronteras observadas?\n",
    "3. Reflexiona sobre las diferencias entre los algoritmos LDA y QDA. 쮼n qu칠 se distinguen en t칠rminos de supuestos, enfoque y resultados en tus datos?\n",
    "\n",
    "Este an치lisis te permitir치 comprender las caracter칤sticas y la eficacia de ambos modelos, LDA y QDA, y c칩mo se aplican a tu conjunto de datos.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pmRdVVp1ssYx"
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gjEyHwXFfHw4"
   },
   "source": [
    "<a id='ej13'></a>\n",
    "## 1.4. KNN (1 punto)\n",
    "\n",
    "En este punto, vas a entender el funcionamiento del algoritmo KNN (K-Nearest-Neighbor), que se basa en la proximidad de los puntos de datos en un espacio de caracter칤sticas. Analizar치s sus ventajas y desventajas, y comprender치s c칩mo los par치metros que lo componen influyen en su comportamiento.\n",
    "\n",
    "KNN es un algoritmo de tipo supervisado basado en instancia. Esto significa:\n",
    "\n",
    "- Supervisado: Tu conjunto de datos de entrenamiento est치 etiquetado con la clase o resultado esperado.\n",
    "- Basado en instancia (_Lazy Learning_): El algoritmo no aprende expl칤citamente un modelo, como en la Regresi칩n Log칤stica o los 치rboles de decisi칩n. En cambio, memoriza las instancias de entrenamiento y las utiliza como \"conocimiento\" en la fase de predicci칩n.\n",
    "\n",
    "Para entender c칩mo funciona KNN, sigue estos pasos:\n",
    "\n",
    "1. Calcula la distancia entre el 칤tem a clasificar y los dem치s 칤tems del dataset de entrenamiento.\n",
    "2. Selecciona los \"k\" elementos m치s cercanos, es decir, aquellos con la menor distancia, seg칰n el tipo de distancia que utilices (eucl칤dea, coseno, manhattan, etc).\n",
    "3. Realiza una \"votaci칩n de mayor칤a\" entre los k puntos seleccionados: la clase que predomine en estos puntos decidir치 la clasificaci칩n final del 칤tem analizado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TADnYVTPfHw4"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Realiza los siguientes pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un clasificador KNN con el hiperpar치metro `n_neighbors=2` usando el clasificador `KNeighborsClassifier` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "\n",
    "Si al entrenar el clasificador aparece un aviso (warning) y deseas ignorarlo, ejecuta el siguiente c칩digo antes del entrenamiento:\n",
    "\n",
    "`import warnings`\n",
    "`warnings.filterwarnings('ignore', message='^.*will change.*$', category=FutureWarning)`\"\n",
    "\n",
    "Esto te permitir치 evaluar la efectividad del modelo KNN con `n_neighbors=2` en tu conjunto de datos, y entender c칩mo se comporta en t칠rminos de clasificaci칩n y separaci칩n de clases.    \n",
    "    \n",
    "    \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQ50-Q15fHw4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NLjdWCM2fHw4"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tF_2dKwAfHw8"
   },
   "source": [
    "En el modelo que has entrenado, has fijado el par치metro `n_neighbors` de forma arbitraria. Sin embargo, es posible que con otro valor obtengas una mejor predicci칩n. Para encontrar el valor 칩ptimo de los par치metros de un modelo (_hyperparameter tunning_), a menudo se utiliza una b칰squeda de rejilla (_grid search_). Esto implica entrenar un modelo para cada combinaci칩n posible de hiperpar치metros y evaluarlo mediante validaci칩n cruzada (_cross validation_) con 5 particiones estratificadas. Luego, seleccionar치s la combinaci칩n de hiperpar치metros que haya obtenido los mejores resultados.\n",
    "\n",
    "En este caso, te centrar치s en optimizar un solo hiperpar치metro:\n",
    "\n",
    "- 洧녲: el n칰mero de vecinos que se consideran para clasificar un nuevo ejemplo. Debes probar con todos los valores entre 1 y 20.\n",
    "\n",
    "Realiza este proceso para identificar el n칰mero 칩ptimo de vecinos, lo que te permitir치 mejorar la precisi칩n de tus predicciones con el modelo KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UDe0lBofHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementaci칩n:</strong>\n",
    "\n",
    "Para calcular el valor 칩ptimo del hiperpar치metro _k_ (`n_neighbors`), debes realizar una b칰squeda de rejilla con validaci칩n cruzada. Este proceso te ayudar치 a encontrar el valor 칩ptimo de _k_. Para cada valor, calcula su promedio y la desviaci칩n est치ndar. Luego, implementa un _heatmap_ para visualizar la precisi칩n seg칰n los diferentes valores del hiperpar치metro.\n",
    "\n",
    "Utiliza el m칩dulo `GridSearchCV` de `sklearn` para calcular el mejor hiperpar치metro. Para la visualizaci칩n del _heatmap_, emplea la librer칤a `Seaborn`.\n",
    "\n",
    "Estos pasos te permitir치n identificar de manera efectiva y visual el valor de _k_ que maximiza la precisi칩n de tu modelo KNN.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qb8GHGHOfHw8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h9Af8JcrfHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EKBqIrRJfHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Sigue estos pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un clasificador KNN utilizando el mejor hiperpar치metro que hayas encontrado.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "\n",
    "Este proceso te permitir치 ver c칩mo el hiperpar치metro 칩ptimo que has identificado mejora la efectividad de tu modelo KNN en la clasificaci칩n de los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mK3awkbwfHw8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hBI2kRH5fHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "teluDtnHfHw8"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Comenta los resultados obtenidos en la b칰squeda del mejor hiperpar치metro. Reflexiona sobre c칩mo vari칩 el rendimiento del modelo con los diferentes valores de `n_neighbors`.\n",
    "2. Analiza c칩mo se visualiza gr치ficamente el cambio del valor de `n_neighbors`. 쯆bservas alguna tendencia o patr칩n claro? 쮼s coherente esta diferencia entre los dos gr치ficos al cambiar el par치metro?\n",
    "3. Examina las fronteras de decisi칩n que has generado. 쯃a forma de estas fronteras tiene sentido dado el algoritmo KNN utilizado? Piensa en c칩mo la elecci칩n del n칰mero de vecinos influye en la forma de la frontera.\n",
    "4. Eval칰a las predicciones realizadas sobre el conjunto de test. Observa su precisi칩n y c칩mo se distribuyen en relaci칩n con las fronteras de decisi칩n. 쯉on estas predicciones consistentes con lo que observas en las fronteras de decisi칩n?\n",
    "\n",
    "Este an치lisis te ayudar치 a comprender la eficacia del modelo KNN con diferentes configuraciones de `n_neighbors` y su impacto en la clasificaci칩n de los datos.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ET55syCqfHw8",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoWjLa8LfHw9"
   },
   "source": [
    "<a id='ej14'></a>\n",
    "## 1.5. SVM (1 punto)\n",
    "\n",
    "En esta secci칩n, vas a explorar las M치quinas de Vectores de Soporte (SVM), que se basan en el concepto del _Maximal Margin Classifier_ y el hiperplano.\n",
    "\n",
    "Un hiperplano en un espacio p-dimensional se define como un subespacio plano y af칤n de dimensiones p-1. En dos dimensiones, es una recta; en tres, un plano convencional. Para dimensiones mayores a tres, aunque no es intuitivo visualizarlo, el concepto se mantiene.\n",
    "\n",
    "Cuando los casos son perfectamente separables de manera lineal, surgen infinitos posibles hiperplanos. Para seleccionar el clasificador 칩ptimo, utiliza el concepto de _maximal margin hyperplane_, el hiperplano que se encuentra m치s alejado de todas las observaciones de entrenamiento. Este se define calculando la distancia perpendicular m칤nima (margen) de las observaciones a un hiperplano. El hiperplano 칩ptimo es aquel que maximiza este margen.\n",
    "\n",
    "En el proceso de optimizaci칩n, debes tener en cuenta que solo las observaciones al margen o que lo violan (vectores soporte) influyen en el hiperplano. Estos vectores soporte son los que definen el clasificador."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WMxzknM5fHw9"
   },
   "source": [
    "#### Los _kernels_ en SVM\n",
    "\n",
    "En situaciones donde no puedes encontrar un hiperplano que separe dos clases, es decir, cuando las clases no son linealmente separables, puedes utilizar el truco del n칰cleo (_kernel trick_). Este m칠todo te permite trabajar en una dimensi칩n nueva donde es posible encontrar un hiperplano para separar las clases.\n",
    "\n",
    "Al igual que con el KNN, las SVM tambi칠n dependen de varios hiperpar치metros. En este caso, te enfocar치s en optimizar dos hiperpar치metros:\n",
    "\n",
    "1. **C**: la regularizaci칩n, que es el valor de penalizaci칩n de los errores en la clasificaci칩n. Este valor indica el compromiso entre obtener el hiperplano con el margen m치s grande posible y clasificar correctamente el m치ximo n칰mero de ejemplos. Debes probar los siguientes valores: 0.01, 0.1, 1, 10, 50, 100 y 200.\n",
    "   \n",
    "2. **Gamma**: un coeficiente que multiplica la distancia entre dos puntos en el kernel radial. En t칠rminos simples, cuanto m치s peque침o sea gamma, m치s influencia tendr치n dos puntos cercanos. Debes probar los valores: 0.001, 0.01, 0.1, 1 y 10.\n",
    "\n",
    "Para validar el rendimiento del algoritmo con cada combinaci칩n de hiperpar치metros, utiliza la validaci칩n cruzada (_cross-validation_) con 4 particiones estratificadas.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UdHJe2affHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementaci칩n:</strong>\n",
    "\n",
    "\n",
    "1. Calcula el valor 칩ptimo de los hiperpar치metros _C_ y _gamma_ utilizando una b칰squeda de rejilla con validaci칩n cruzada. Este proceso te ayudar치 a encontrar los valores 칩ptimos.\n",
    "2. Para cada combinaci칩n de valores, calcula su promedio y la desviaci칩n est치ndar.\n",
    "3. Haz un _heatmap_ para visualizar la precisi칩n seg칰n los diferentes valores de los hiperpar치metros.\n",
    "\n",
    "Utiliza el m칩dulo `GridSearchCV` de `sklearn` para calcular los mejores hiperpar치metros con el clasificador SVC (de `SVM` de `sklearn`). Para la visualizaci칩n del _heatmap_, emplea la librer칤a `Seaborn`.\n",
    "\n",
    "Estos pasos te permitir치n identificar de manera efectiva y visual los valores de _C_ y _gamma_ que maximizan la precisi칩n de tu modelo SVM.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBxQod7LfHw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em56OnxofHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_E-ae6x0fHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Realiza los siguientes pasos con el dataset de entrenamiento (_train_):\n",
    "\n",
    "1. Entrena un modelo SVM utilizando la mejor combinaci칩n de par치metros que hayas encontrado.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "\n",
    "Este proceso te permitir치 ver c칩mo la mejor combinaci칩n de par치metros mejora la efectividad de tu modelo SVM en la clasificaci칩n de los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qf2kQHuMfHw9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHfaJpAfHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oT7FOpk9fHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Comenta los resultados obtenidos en la b칰squeda de los mejores hiperpar치metros. Reflexiona sobre c칩mo vari칩 el rendimiento del modelo SVM con los diferentes valores de _C_ y _gamma_. Considera si los valores 칩ptimos encontrados tienen sentido en el contexto de tu conjunto de datos.\n",
    "2. Examina las fronteras de decisi칩n que has generado con el modelo SVM. 쯃a forma de estas fronteras es coherente con lo que esperar칤as del algoritmo utilizado? Piensa en c칩mo la combinaci칩n de hiperpar치metros seleccionados podr칤a influir en la forma de las fronteras.\n",
    "3. Eval칰a las predicciones realizadas sobre el conjunto de test. Observa su precisi칩n y c칩mo se distribuyen en relaci칩n con las fronteras de decisi칩n. 쯉on estas predicciones consistentes con lo que observas en las fronteras de decisi칩n?\n",
    "\n",
    "Este an치lisis te ayudar치 a comprender la eficacia del modelo SVM con los hiperpar치metros seleccionados y su impacto en la clasificaci칩n de los datos.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kan9R0kpfHw9",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9Z1MzFbfHw9"
   },
   "source": [
    "<a id='ej15'></a>\n",
    "## 1.6. 츼rboles de decisi칩n (1 punto)\n",
    "\n",
    "En esta secci칩n, vas a explorar los 치rboles de decisi칩n, modelos predictivos que se basan en reglas binarias (si/no) para clasificar las observaciones seg칰n sus atributos y predecir el valor de la variable respuesta. Estos 치rboles pueden ser clasificadores, como en tu ejemplo, o regresores para predecir variables continuas.\n",
    "\n",
    "#### Construcci칩n de un 츼rbol\n",
    "\n",
    "Para construir un 치rbol, sigue el algoritmo de *recursive binary splitting*:\n",
    "\n",
    "1. Comienza en la parte superior del 치rbol, donde todas las observaciones pertenecen a la misma regi칩n.\n",
    "2. Identifica todos los posibles puntos de corte para cada uno de los predictores. Estos puntos de corte son los diferentes niveles de los predictores.\n",
    "3. Eval칰a las posibles divisiones para cada predictor utilizando una medida espec칤fica. En los clasificadores, estas medidas pueden ser el *classification error rate*, el 칤ndice Gini, la entrop칤a o el chi-square.\n",
    "\n",
    "Comprender estos pasos te ayudar치 a entender c칩mo los 치rboles de decisi칩n crean divisiones binarias para clasificar los datos y c칩mo estos pueden aplicarse tanto para clasificaci칩n como para regresi칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTnIbWTYfHw9"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Sigue estos pasos:\n",
    "\n",
    "1. Con el dataset de entrenamiento (_train_), entrena un 치rbol de decisi칩n utilizando el clasificador `DecisionTreeClassifier` de la biblioteca `tree` de `sklearn`.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "5. Representa el 치rbol de decisi칩n. Puedes utilizar el comando `plot.tree` de la biblioteca `tree` de `sklearn`.\n",
    "\n",
    "Estos pasos te permitir치n evaluar c칩mo el 치rbol de decisi칩n se comporta en tu conjunto de datos, tanto en t칠rminos de clasificaci칩n como en su representaci칩n visual.\"\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqqvClsEfHw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zpPKYCZjfHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PI_q3bCafHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Eval칰a y comenta los resultados obtenidos con el 치rbol de decisi칩n. Considera tanto el _accuracy_ del modelo en los conjuntos de _train_ y _test_ como los resultados de la matriz de confusi칩n.\n",
    "2. Reflexiona sobre c칩mo la frontera de decisi칩n visualizada en el conjunto de _test_ se alinea con los resultados obtenidos. 쮼s coherente con lo que esperar칤as de un 치rbol de decisi칩n?\n",
    "3. Observa la representaci칩n gr치fica del 치rbol. Analiza c칩mo las diferentes ramificaciones y decisiones tomadas en el 치rbol explican el comportamiento del modelo y su impacto en la clasificaci칩n de los datos.\n",
    "\n",
    "Este an치lisis te ayudar치 a comprender en profundidad el funcionamiento y la eficacia del 치rbol de decisi칩n en tu conjunto de datos espec칤fico.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MABFtjLJfHw-",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlU7Tg-1fHw-"
   },
   "source": [
    "#### Evitando el *overfitting*\n",
    "\n",
    "El proceso de construcci칩n de 치rboles descrito tiende a reducir r치pidamente el error de entrenamiento, por lo que generalmente el modelo se ajusta muy bien a las observaciones utilizadas como entrenamiento (conjunto de *train*). Como consecuencia, los 치rboles de decisi칩n tienden al *overfitting*.\n",
    "   \n",
    "Para evitar el *overfitting* en los 치rboles de decisi칩n, es crucial que modifiques ciertos hiperpar치metros del modelo de la siguiente manera:\n",
    "\n",
    "1. Utiliza el hiperpar치metro `max_depth`, que define la profundidad m치xima del 치rbol. Deber치s explorar los valores entre 4 y 10 para encontrar el equilibrio adecuado entre la complejidad del modelo y su capacidad para generalizar.\n",
    "2. Establece el hiperpar치metro `min_samples_split`, que es el n칰mero m칤nimo de observaciones que debe tener una hoja del 치rbol antes de considerar una divisi칩n. Experimenta con valores como 2, 10, 20, 50 y 100 para asegurarte de que el 치rbol no se vuelva demasiado espec칤fico para las observaciones de entrenamiento.\n",
    "\n",
    "Ajustando estos hiperpar치metros, podr치s controlar la tendencia del 치rbol de decisi칩n a sobreajustarse al conjunto de entrenamiento, mejorando as칤 su capacidad para realizar predicciones efectivas en nuevos datos.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o2KF6CI2fHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <strong>Implementaci칩n:</strong>\n",
    "\n",
    "1. Calcula el valor 칩ptimo de los hiperpar치metros `max_depth` y `min_samples_split` utilizando una b칰squeda de rejilla con validaci칩n cruzada. Este proceso te ayudar치 a encontrar los valores 칩ptimos que evitar치n el sobreajuste.\n",
    "2. Para cada combinaci칩n de valores, calcula su promedio y la desviaci칩n est치ndar.\n",
    "3. Haz un _heatmap_ para visualizar la precisi칩n seg칰n los diferentes valores de los hiperpar치metros.\n",
    "\n",
    "Utiliza el m칩dulo `GridSearchCV` de `sklearn` para calcular los mejores hiperpar치metros con el clasificador `DecisionTreeClassifier` de `tree` de `sklearn`. Para la visualizaci칩n del _heatmap_, emplea la librer칤a `Seaborn`.\n",
    "\n",
    "Estos pasos te permitir치n identificar de manera efectiva y visual los valores de `max_depth` y `min_samples_split` que maximizan la precisi칩n de tu 치rbol de decisi칩n, minimizando el riesgo de sobreajuste.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JAaYYYTbfHw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RcaZzrK-fHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DT9ttV8GfHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "\n",
    "1. Entrena un 치rbol de decisi칩n con el dataset de entrenamiento (_train_) utilizando la mejor combinaci칩n de par치metros que hayas encontrado.\n",
    "2. Calcula el _accuracy_ (precisi칩n) del modelo tanto en los datos de _train_ como de _test_.\n",
    "3. Calcula la matriz de confusi칩n utilizando los datos de _test_.\n",
    "4. Representa gr치ficamente la frontera de decisi칩n con los datos de _test_.\n",
    "5. Representa el 치rbol de decisi칩n.\n",
    "\n",
    "Estos pasos te permitir치n evaluar c칩mo el 치rbol de decisi칩n, ajustado con los hiperpar치metros 칩ptimos, se comporta en tu conjunto de datos, tanto en t칠rminos de clasificaci칩n como en su representaci칩n visual.\"\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R3ZJ7I-FfHw-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xRxJyG5MfHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-cQKFNpffHw-"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Eval칰a y comenta los resultados obtenidos en la b칰squeda de los mejores hiperpar치metros. Considera c칩mo la combinaci칩n 칩ptima de `max_depth` y `min_samples_split` ha impactado el rendimiento del 치rbol de decisi칩n.\n",
    "2. Examina las fronteras de decisi칩n generadas con el conjunto de _test_. Reflexiona sobre si la forma de estas fronteras es coherente con lo que esperar칤as de un 치rbol de decisi칩n configurado con estos hiperpar치metros.\n",
    "3. Analiza las predicciones realizadas sobre el conjunto de test. Observa su precisi칩n y c칩mo se distribuyen en relaci칩n con las fronteras de decisi칩n. 쯉on consistentes estas predicciones con la estructura del 치rbol de decisi칩n y las fronteras observadas?\n",
    "\n",
    "Este an치lisis te ayudar치 a comprender la eficacia del 치rbol de decisi칩n con los hiperpar치metros seleccionados y su impacto en la clasificaci칩n de los datos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3vL6gWjfHw_",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1N8TJl-1fHw_"
   },
   "source": [
    "<a id='eje2'></a>\n",
    "# 2. Implementaci칩n del caso pr치ctico (5 puntos)\n",
    "\n",
    "Hoy en d칤a, la log칤stica de la 칰ltima milla es un problema abordado en la industria por muchas empresas dedicadas al comercio electr칩nico. La informaci칩n proporcionada al usuario a la hora de realizar un pedido puede suponer un valor diferencial. Por ello, muchas empresas dedican muchos recursos para dar una estimaci칩n precisa sobre el tiempo que va a tardar en llegar cada pedido. En este ejercicio nos vamos a centrar en predecir el nivel de servicio de las operaciones log칤sticas de 칰ltima milla de Amazon. En concreto identificaremos aquellas entregas que se consideren premium (tiempo de reparto inferior a dos horas).\n",
    "\n",
    "Para ello, vamos a utilizar el conjunto de datos de entregas [amazon-delivery-dataset](https://www.kaggle.com/datasets/sujalsuthar/amazon-delivery-dataset), el cual incluye datos sobre m치s de 43.632 entregas en varias ciudades, con informaci칩n relevante sobre los detalles del pedido, los agentes de entrega, las condiciones meteorol칩gicas y del tr치fico, y las m칠tricas de rendimiento de la entrega. En concreto, el dataset contiene 16 caracter칤sticas:\n",
    "\n",
    "- Order_ID: identificador 칰nico de pedido\n",
    "- Agent_Age: edad del agente (repartidor)\n",
    "- Agent_Rating: puntuaci칩n del agente (repartidor)\n",
    "- Store_Latitude: latitud del almac칠n o tienda\n",
    "- Store_Longitude: longitud del almac칠n o tienda\n",
    "- Drop_Latitude: latitud del cliente\n",
    "- Drop_Longitude: longitud del cliente\n",
    "- Order_Date: fecha del pedido\n",
    "- Order_Time: hora del pedido \n",
    "- Pickup_Time: hora a la que el pedido fue recogido para su entrega\n",
    "- Weather: informaci칩n sobre la climatolog칤a\n",
    "- Traffic: informaci칩n sobre el tr치fico\n",
    "- Vehicle: informaci칩n sobre el veh칤culo\n",
    "- Area: informaci칩n sobre el 치rea de reparto\n",
    "- Category: categor칤a de los productos del pedido\n",
    "- Delivery_Time: tiempo de reparto (minutos)\n",
    "\n",
    "El objetivo de esta secci칩n es abordar el an치lisis de este conjunto de datos y entrenar una red neuronal (Perceptr칩n Multicapa) para predecir el nivel de servicio. Aqu칤 tienes algunos pasos que podr칤as seguir:\n",
    "\n",
    "1. **An치lisis Exploratorio de Datos (EDA)**: Comienza explorando el conjunto de datos para comprender su estructura y distribuci칩n. Analiza la proporci칩n de cada clase. Observa la distribuci칩n de las diferentes caracter칤sticas y su relaci칩n con la clase objetivo \"class\".\n",
    "\n",
    "2. **Preprocesamiento de Datos**: Considera normalizar las caracter칤sticas para que est칠n en la misma escala que las componentes principales.\n",
    "\n",
    "3. **Modelizaci칩n**: Utiliza un perceptr칩n multicapa como herramienta de clasificaci칩n. Dado que el objetivo es identificar el nivel de servicio de la entrega, es vital centrarse en m칠tricas como la precisi칩n, la sensibilidad (recall), el valor F1 y el 치rea bajo la curva ROC (AUC-ROC).\n",
    "\n",
    "4. **Evaluaci칩n**: Realiza una evaluaci칩n y an치lisis riguroso del rendimiento de tu modelo.\n",
    "\n",
    "Este enfoque integral te permitir치 no solo construir un modelo efectivo sino tambi칠n comprender mejor las caracter칤sticas subyacentes del nivel de servicio en el conjunto de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAn2GmJcfHw_"
   },
   "source": [
    "<a id='ej20'></a>\n",
    "## 2.1. Carga de datos y procesamiento inicial (0.5 puntos)\n",
    "\n",
    "Lo primero que debes hacer es cargar el conjunto de datos y visualizar informaci칩n relevante del mismo. Aseg칰rate de verificar lo siguiente:\n",
    "\n",
    "1. Confirma la cantidad total de filas y columnas en el DataFrame.\n",
    "2. Revisa el nombre de cada columna del DataFrame.\n",
    "3. Verifica el n칰mero de valores no nulos en cada columna.\n",
    "4. Identifica el tipo de datos de cada columna, que puede ser int, float, object, entre otros.\n",
    "5. Comprueba la cantidad de memoria utilizada por el DataFrame.\n",
    "\n",
    "Estos pasos te proporcionar치n una comprensi칩n inicial clara y detallada del conjunto de datos con el que est치s trabajando."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_path = kagglehub.dataset_download(\"sujalsuthar/amazon-delivery-dataset\")\n",
    "dataset_path = os.path.join(root_path, \"amazon_delivery.csv\") \n",
    "data = pd.read_csv(dataset_path)\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "C칩mo se puede observar, no disponemos del nivel de servicio en el conjunto de datos. Por ello, vamos a definir que todas las entregas realizadas en un m치ximo de dos horas han tenido un servicio Premium. Para ello, crea una nueva columna denominada \"Premium_Delivery\", que contenga el valor 1 si la entrega se ha realizado en un m치ximo de 120 minutos, y un 0 en caso contrario. Es importante asegurar que el tipo de la nueva columna sea entero.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Para simplificar el ejercicio y facilitar su comprensi칩n, se deben eliminar las siguientes columnas: Order_ID, Order_Date, Order_Time y Pickup_Time\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "    \n",
    "A continuaci칩n, vamos a calcular la distancia harvesiana en kil칩metros entre el almac칠n y el cliente. Para ello, debemos crear un nueva columna \"Distance\" y eliminar las cuatro columnas relacionadas con las coordenadas.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nwJqa9WIfHw_"
   },
   "source": [
    "<a id='ej21'></a>\n",
    "## 2.2. An치lisis Exploratorio de Datos (EDA) (1.25 puntos)\n",
    "\n",
    "El An치lisis Exploratorio de Datos (EDA, por sus siglas en ingl칠s) en ciencia de datos es un enfoque inicial para comprender y resumir el contenido de un conjunto de datos. Este proceso implica varias t칠cnicas y pasos:\n",
    "\n",
    "1. **Inspecci칩n de Datos**: Se comienza por revisar los datos brutos para identificar su estructura, tama침o y tipo (como num칠rico, categ칩rico). Esto incluye detectar valores faltantes o inusuales.\n",
    "\n",
    "2. **Resumen Estad칤stico**: Se calculan estad칤sticas descriptivas como la media, mediana, rango, varianza y desviaci칩n est치ndar para obtener una idea general de las tendencias y patrones en los datos.\n",
    "\n",
    "3. **Visualizaci칩n de Datos**: Se utilizan gr치ficos y diagramas (como histogramas, gr치ficos de caja, diagramas de dispersi칩n) para visualizar distribuciones, relaciones entre variables y posibles anomal칤as. Esto ayuda a comprender mejor los datos y a identificar patrones o irregularidades.\n",
    "\n",
    "4. **An치lisis de Relaciones y Correlaciones**: Se exploran las relaciones entre diferentes variables para entender c칩mo se influencian entre s칤. Esto puede implicar el uso de matrices de correlaci칩n y gr치ficos de dispersi칩n.\n",
    "\n",
    "5. **Identificaci칩n de Patrones y Anomal칤as**: Se buscan patrones consistentes o anomal칤as (como valores at칤picos) que puedan sugerir tendencias o problemas en los datos.\n",
    "\n",
    "El EDA es una fase cr칤tica en cualquier proyecto de ciencia de datos, ya que proporciona una comprensi칩n profunda y una base s칩lida para posteriores an치lisis y modelado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pIX-1GizfHw_",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "1. Calcula las frecuencias de la variable objetivo (`Premium_Delivery`) en tu conjunto de datos. \n",
    "2. Crea un gr치fico de barras para visualizar estas frecuencias. Esto te ayudar치 a entender la proporci칩n de entregas premium en comparaci칩n con las que no lo son.\n",
    "\n",
    "A continuaci칩n, analiza la distribuci칩n de las variables num칠ricas:\n",
    "\n",
    "1. Representa gr치ficamente el histograma de las variables, separando las observaciones seg칰n la clase a la que pertenecen (premium o no).\n",
    "2. Organiza todos los histogramas en un formato de 4 filas y 1 columna. Esto facilitar치 la comparaci칩n visual de las distribuciones para cada clase en cada variable.\n",
    "\n",
    "Por 칰ltimo, analiza la distribuci칩n de las variables categ칩ricas de forma an치loga a las variables num칠ricas, organizando todos los histogramas en un formato de 5 filas y 1 columna.\n",
    "\n",
    "Estos pasos te permitir치n obtener una visi칩n m치s clara de la estructura de tu conjunto de datos y c칩mo las diferentes variables pueden influir en la identificaci칩n de las entregas premium.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mBJU0PIHfHw_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-MFH2nvWfHw_"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rDxe75IfHw_"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "1. Eval칰a la relaci칩n de las frecuencias de la variable `Premium_Delivery`. Reflexiona sobre c칩mo se distribuyen las transacciones entre premium y no no premium. 쮼s la distribuci칩n significativamente desigual? 쯈u칠 implica esto para el an치lisis y la modelizaci칩n de los datos?\n",
    "2. Analiza la informaci칩n proporcionada por los histogramas de las variables descriptoras. Observa si hay diferencias notables en las distribuciones de estas variables entre las clases. Preg칰ntate: 쮿ay variables que muestren patrones distintos para el nivel de servicio?\n",
    "3. Considera si hay otras formas de visualizaci칩n que podr칤an ser 칰tiles para entender mejor los datos. Por ejemplo, 쯥er칤an 칰tiles los diagramas de caja (boxplots) para visualizar la distribuci칩n de las variables en ambas clases? 쯇odr칤a un mapa de calor de la matriz de correlaci칩n entre variables ayudarte a entender las relaciones entre ellas?\n",
    "\n",
    "Este an치lisis te ayudar치 a obtener una comprensi칩n m치s profunda de la naturaleza de tus datos y a identificar posibles caracter칤sticas que podr칤an ser importantes para detectar las entregas premium.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oHW_jgcbfHw_",
    "tags": []
   },
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oa3HbpukfHw_"
   },
   "source": [
    "<a id='ej22'></a>\n",
    "## 2.3. Preprocesamiento de Datos (1.25 puntos)\n",
    "\n",
    "El preprocesamiento de datos en ciencia de datos es un paso crucial que involucra la preparaci칩n y transformaci칩n de datos brutos en un formato adecuado para su posterior an치lisis y modelado. Este proceso incluye varias tareas esenciales:\n",
    "\n",
    "1. **Limpieza de Datos**: Se eliminan o corrigen datos err칩neos, incompletos, inexactos o irrelevantes. Esto puede incluir tratar con valores faltantes, corregir errores de entrada y manejar outliers.\n",
    "\n",
    "2. **Normalizaci칩n y Escalado**: Los datos se transforman para que est칠n en una escala com칰n, sin distorsionar diferencias en los rangos de valores ni perder informaci칩n. Por ejemplo, escalado min-max o estandarizaci칩n.\n",
    "\n",
    "3. **Codificaci칩n de Variables Categ칩ricas**: Las variables categ칩ricas (como g칠nero o pa칤s) se convierten en formatos num칠ricos para que puedan ser procesadas por algoritmos de aprendizaje autom치tico, utilizando t칠cnicas como codificaci칩n one-hot o codificaci칩n de etiquetas.\n",
    "\n",
    "4. **Divisi칩n de Datos**: Los datos se dividen en conjuntos de entrenamiento, validaci칩n y prueba, permitiendo entrenar modelos, afinar hiperpar치metros y evaluar el rendimiento del modelo de manera efectiva.\n",
    "\n",
    "5. **Manejo de Datos Desbalanceados**: En casos de conjuntos de datos desbalanceados, se aplican t칠cnicas como sobremuestreo o submuestreo para asegurar que el modelo no est칠 sesgado hacia la clase m치s frecuente.\n",
    "\n",
    "6. **Ingenier칤a de Caracter칤sticas**: Se crean nuevas variables (caracter칤sticas) a partir de los datos existentes para mejorar la capacidad del modelo para aprender patrones y hacer predicciones.\n",
    "\n",
    "El preprocesamiento es esencial para mejorar la calidad de los datos y hacerlos m치s adecuados y efectivos para an치lisis y modelado en proyectos de ciencia de datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong> elimina los atributos categ칩ricos del conjunto de datos y en su lugar introduce la transformaci칩n de dichos atributos a tantas variables binarias como categor칤as tengan. Es importante que las nuevas columnas generadas sean de tipo entero. Recuerda que la codificaci칩n one-hot convierte las etiquetas categ칩ricas en vectores binarios. En estos vectores, el valor de 1 se asigna a la posici칩n correspondiente a la clase y el valor de 0 a todas las dem치s posiciones. Esto facilita que los modelos de aprendizaje autom치tico procesen y entiendan las etiquetas categ칩ricas.\n",
    "<hr>\n",
    "Sugerencia: utilizad la funci칩n \"get_dummies\" de \"pandas\".\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "En la primera secci칩n del ejercicio pudimos observar que la columna Agent_Rating ten칤a un n칰mero reducido de valores nulos. En este ejercicio tenemos que imputar los valores nulos por el m칤nimo valor de la columna y verificar que ninguna columna adicional tiene valores nulos.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iu0SHBWbfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Ahora vamos a realizar la divisi칩n del conjunto de datos, para ello sigue estos pasos:\n",
    "\n",
    "1. Separa los descriptores de la variable respuesta. Asigna los descriptores al conjunto `X` y la variable respuesta al conjunto `y`.\n",
    "2. Elimina del conjunto de descriptores la columna `Delivery_Time`, dado que fue la que utilizamos para calcular nuestra variable respuesta.\n",
    "3. Divide el _dataset_ en dos subconjuntos: uno para entrenamiento (_train_) y otro para pruebas (_test_). Asigna el 80% de los datos al conjunto de entrenamiento (`X_train`, `y_train`) y el 20% al conjunto de pruebas (`X_test`, `y_test`). Utiliza la funci칩n `train_test_split` de la biblioteca `model_selection` de `sklearn`. Aseg칰rate de usar `random_state = 24` y haz una divisi칩n estratificada para mantener la misma proporci칩n de clases en ambos conjuntos.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lj__HV48fHxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0e2C3NzfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKCW1Rh1fHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "1. Normaliza los descriptores utilizando el `StandardScaler` de `sklearn`. Esto estandarizar치 las caracter칤sticas restando la media y dividiendo por la desviaci칩n est치ndar.\n",
    "2. Muestra las dimensiones del conjunto de descriptores original, del conjunto de entrenamiento y del conjunto de prueba. Esto te permitir치 ver c칩mo se han dividido los datos.\n",
    "\n",
    "<strong>Nota:</strong> Ajusta el `StandardScaler` 칰nicamente con los descriptores de entrenamiento para evitar la fuga de informaci칩n o 'data leakage'. La fuga de informaci칩n ocurre cuando se utiliza informaci칩n del conjunto de prueba o validaci칩n en el proceso de ajuste del modelo. Es decir, si ajustas el modelo de escalado con todo el conjunto de datos, estar칤as utilizando informaci칩n del conjunto de prueba o validaci칩n en el ajuste, lo que podr칤a dar la impresi칩n de que el modelo es m치s preciso de lo que realmente es. Por lo tanto, aseg칰rate de ajustar el `StandardScaler` solo con los datos de entrenamiento y luego aplicarlo a los conjuntos de entrenamiento y prueba.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vrgdxoZZfHxA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxe-T3-mfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Convierte los conjunto de datos de entrenamiento y test en tensores, utilizando el m칠todo `tensor` de la librer칤a `PyTorch`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cpg-RIGpfHxA"
   },
   "source": [
    "<a id='ej23'></a>\n",
    "## 2.4. Modelizaci칩n (2 puntos)\n",
    "\n",
    "El MLP (Perceptr칩n Multicapa) es, sin duda, una poderosa herramienta en el campo del aprendizaje autom치tico y la inteligencia artificial. Puede manejar tareas de clasificaci칩n y regresi칩n, lo que lo hace vers치til para una variedad de problemas. Su capacidad para modelar relaciones no lineales complejas lo convierte en una elecci칩n popular cuando los datos no siguen patrones lineales simples.\n",
    "\n",
    "Aqu칤 hay algunos puntos clave sobre el MLP:\n",
    "\n",
    "- **Capas y Neuronas**: El MLP consta de m칰ltiples capas de neuronas, que incluyen una capa de entrada, una o m치s capas ocultas y una capa de salida. Cada neurona en una capa est치 conectada a todas las neuronas en la capa siguiente.\n",
    "\n",
    "- **Funciones de Activaci칩n**: Para introducir no linealidad en el modelo, se utilizan funciones de activaci칩n en las neuronas, como la funci칩n sigmoide, ReLU (Rectified Linear Unit) o tangente hiperb칩lica. Estas funciones permiten al MLP capturar patrones complejos en los datos.\n",
    "\n",
    "- **Aprendizaje Supervisado**: El entrenamiento del MLP implica ajustar los pesos de las conexiones entre neuronas para minimizar la diferencia entre las salidas producidas por la red y las salidas deseadas. Esto se hace utilizando algoritmos de aprendizaje supervisado, como el descenso del gradiente.\n",
    "\n",
    "- **Ajuste de Hiperpar치metros**: Al igual que otros modelos de aprendizaje autom치tico, el MLP tiene hiperpar치metros importantes, como el n칰mero de capas ocultas, el n칰mero de neuronas en cada capa, la funci칩n de activaci칩n y la tasa de aprendizaje. A menudo, es necesario ajustar estos hiperpar치metros para obtener un buen rendimiento en una tarea espec칤fica.\n",
    "\n",
    "- **Generalizaci칩n**: Uno de los desaf칤os en el entrenamiento de MLP es evitar el sobreajuste (overfitting), donde el modelo se adapta demasiado a los datos de entrenamiento y no generaliza bien a datos nuevos. La regularizaci칩n y la validaci칩n cruzada son t칠cnicas comunes para abordar este problema.\n",
    "\n",
    "En este contexto, el MLP puede ser una excelente opci칩n para modelar patrones complejos que indiquen cuando una entrega ser치 premium. Sin embargo, es importante ajustar y evaluar cuidadosamente el modelo para garantizar que funcione de manera efectiva en esta tarea cr칤tica.\n",
    "\n",
    "Crear y entrenar un MLP con varias capas ocultas con funci칩n de activaci칩n ReLU es una excelente elecci칩n. La funci칩n de activaci칩n ReLU (Rectified Linear Unit) es com칰nmente utilizada en capas ocultas de redes neuronales debido a su capacidad para introducir no linealidad en el modelo, lo que le permite aprender patrones complejos en los datos.\n",
    "\n",
    "Por otra parte, el enfoque de apilar capas lineales utilizando la clase `Linear` de PyTorch es una forma eficaz y sencilla de construir modelos de redes neuronales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, X_train, y_train, X_val, y_val, n_epochs, batch_size):\n",
    "    batch_start = torch.arange(0, len(X_train), batch_size)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "    loss_fn = nn.BCELoss()\n",
    "\n",
    "    best_acc = - np.inf\n",
    "    best_weights = None\n",
    "\n",
    "    train_loss_hist = []\n",
    "    train_acc_hist = []\n",
    "    val_loss_hist = []\n",
    "    val_acc_hist = []\n",
    " \n",
    "    for epoch in range(n_epochs):\n",
    "        epoch_loss = []\n",
    "        epoch_acc = []\n",
    "        model.train()\n",
    "        \n",
    "        with tqdm.tqdm(batch_start, unit=\"batch\", mininterval=0, disable=True) as bar:\n",
    "            bar.set_description(f\"Epoch {epoch}\")\n",
    "            for start in bar:\n",
    "\n",
    "                # take a batch\n",
    "                X_batch = X_train[start:start+batch_size]\n",
    "                y_batch = y_train[start:start+batch_size]\n",
    "\n",
    "                # forward pass\n",
    "                y_pred = model(X_batch)\n",
    "                loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                # backward pass\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                # update weights\n",
    "                optimizer.step()\n",
    "\n",
    "                # compute and store metrics\n",
    "                acc = (y_pred.round() == y_batch).float().mean()\n",
    "                epoch_loss.append(float(loss))\n",
    "                epoch_acc.append(float(acc))\n",
    "                bar.set_postfix(\n",
    "                    loss=float(loss),\n",
    "                    acc=float(acc)\n",
    "                )\n",
    "\n",
    "        # Evaluating the model at the end of each epoch\n",
    "        model.eval()\n",
    "        y_pred = model(X_val)\n",
    "        ce = float(loss_fn(y_pred, y_val))\n",
    "        acc = float((y_pred.round() == y_val).float().mean())\n",
    "\n",
    "        train_loss_hist.append(np.mean(epoch_loss))\n",
    "        train_acc_hist.append(np.mean(epoch_acc))\n",
    "        val_loss_hist.append(ce)\n",
    "        val_acc_hist.append(acc)\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_weights = copy.deepcopy(model.state_dict())\n",
    "        # print(f\"Epoch {epoch} validation: Cross-entropy={ce:.2f}, Accuracy={acc*100:.1f}%\")\n",
    "\n",
    "    model.load_state_dict(best_weights)\n",
    "    return best_acc, train_loss_hist, val_loss_hist, train_acc_hist, val_acc_hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SsMGXeDEfHxA"
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "    \n",
    "La modealizaci칩n del MLP la vamos a realizar con la librer칤a `PyTorch`. Para ello:\n",
    "\n",
    "1. Comienza creando el modelo `BinaryServiceLevel`, para lo cual es necesario crear una clase que herede de `nn.Module`.\n",
    "2. En el constructor (`__init__`), declara las siguientes capas:\n",
    "    - Una capa lineal `nn.Linear` de entrada con un tama침o de salida de 19, y una funci칩n de activaci칩n ReLu `nn.ReLu`.\n",
    "    - Una capa lineal `nn.Linear` con un tama침o de salida de 19, y una funci칩n de activaci칩n ReLu `nn.ReLu`.\n",
    "    - Una capa lineal `nn.Linear` de salida con una funci칩n de activaci칩n Sigmoid `nn.Sigmoid`.\n",
    "3. Despu칠s, en el m칠todo `forward` enlaza las diferentes capas y sus respectivas funciones de activaci칩n en el orden definido en el punto anterior. \n",
    "4. No olvides mostrar el n칰mero de par치metros utilizando el m칠todo `.parameters()` del modelo.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "27hz7RN_fHxA"
   },
   "outputs": [],
   "source": [
    "class BinaryServiceLevelBase(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    " \n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "  \n",
    "1. Ahora, es hora de entrenar y validar el modelo aplicando validaci칩n cruzada utilizando `StratifiedKFold` sobre el conjunto de entrenamiento, con un valor de k = 5 y shuffle = True.\n",
    "2. En cada split, el modelo se debe entrenar utilizando la funci칩n `train_model`. Aseg칰rate de entrenar con los datos de entrenamiento y validaci칩n de cada split, establece el n칰mero de 칠pocas en 15 y el tama침o del lote en 32.\n",
    "3. En cada iteraci칩n se deben calcular las siguientes m칠tricas:\n",
    "    - Calcula la exactitud (accuracy) para medir la exactitud de las predicciones.\n",
    "    - Calcula el valor F1, que es una medida que combina exactitud y sensibilidad.\n",
    "    - Calcula el 치rea bajo la curva ROC (AUC-ROC) para evaluar el rendimiento del modelo en la clasificaci칩n binaria.\n",
    "4. Por 칰ltimo, se debe mostrar la media de cada de las m칠tricas calculadas en el punto anterior.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "    \n",
    "1. Realiza un an치lisis de los resultados y decide si consideras que este modelo es aceptable.\n",
    "2. Eval칰a cu치l de las medidas de rendimiento utilizadas es la m치s apropiada.\n",
    "3. Examina la distribuci칩n de las clases y plantea una estrategia, si es necesario, para asegurar la confiabilidad del estudio realizado.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "  \n",
    "Ahora, es hora de analizar la fase de entrenamiento, para ello:\n",
    " \n",
    "1. Divide el dataset de entrenamiento en dos subconjuntos a su vez: uno para entrenamiento (train) y otro para validaci칩n (val), asignando el 80% de los datos al conjunto de entrenamiento.\n",
    "2. Entrena el modelo con la funci칩n `model_train`, guardando todos los par치metros que devuelve.\n",
    "3. Crea gr치ficos que muestren la p칠rdida (`loss`) tanto en el entrenamiento como en la validaci칩n a lo largo de las 칠pocas.\n",
    "4. Por 칰ltimo, genera gr치ficos que representen la exactitud (`accuracy`) en el entrenamiento y la validaci칩n a lo largo de las 칠pocas.\n",
    "\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "\n",
    "쯈ue conclusiones puedes obtener de las gr치ficas tanto de la p칠rdida (`loss`) como de la exactitud (`accuracy`) en el entrenamiento y la validaci칩n a lo largo de las 칠pocas?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "  \n",
    "Es hora evaluar el rendimiento del modelo en el conjunto de test. Para ello:\n",
    "   \n",
    "1. Realiza la predicci칩n sobre el conjunto de test.\n",
    "2. Calcula las m칠tricas de los apartados anteriores: accuracy, f1 score y curva roc.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>Implementaci칩n:</strong>\n",
    "\n",
    "Para analizar el modelo entrenado, nos vamos a apoyar en los [Shapley values](https://en.wikipedia.org/wiki/Shapley_value), los cuales nos introducen a la explicaci칩n de modelos de aprendizaje autom치tico. El objetivo es explicar la predicci칩n de un modelo calculando la contribuci칩n de cada caracter칤stica a la predicci칩n. La explicaci칩n t칠cnica del concepto de SHAP es el c치lculo de los valores de Shapley a partir de la teor칤a de juegos. En pocas palabras, los valores de Shapley son un m칠todo para mostrar el impacto relativo de cada caracter칤stica (o variable) que estamos midiendo en el resultado final del modelo de aprendizaje autom치tico comparando el efecto relativo de las entradas con la media.\n",
    "\n",
    "Para calcular los _shap values_:\n",
    "\n",
    "1. Selecciona una muestra de 10000 registros del conjunto de entrenamiento.\n",
    "2. Inicializa el 'explainer' `shap.DeepExplainer` con el modelo entrenado y la muestra anterior.\n",
    "3. Selecciona una muestra de 400 registros del conjunto de entrenamiento.\n",
    "4. Calcula los _shap_ values utilizando la muestra anterior\n",
    "5. Define y muestra un DataFrame con tres columnas\n",
    "   - Media aritm칠tica del valor absoluto de los valores\n",
    "   - Desviaci칩n t칤pica del valor absoluto de los valores\n",
    "   - Nombre del atributo descriptivo\n",
    "6. Muestra la representaci칩n gr치fica de los valores utilizando la librer칤a shap.\n",
    " \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<strong>Soluci칩n:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "    \n",
    "Relaciona la interpretaci칩n de los _shap values_ con el an치lisis exploratorio de los datos realizado en el ejercicio 2.2.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<strong>An치lisis:</strong>\n",
    "    \n",
    "Imagina que calculamos los _shap values_ para cada split en el ejercicio en el que entrenamos el modelo utilizando validaci칩n cruzada. \n",
    "- 쯃os an치lisis de los diferentes modelos deber칤an ser similares? 쯇or qu칠 o por qu칠 no?\n",
    "- 쯈u칠 indicar칤a si el an치lisis de cada modelo var칤a mucho de uno a otro?\n",
    "- 쯈u칠 usos adicionales les podemos dar a los _shap values_?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "<strong>Respuesta:</strong>\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
